{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce927f17",
   "metadata": {},
   "source": [
    "### Implementing RAG with AutoGen - Agent and Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41cb8b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient, OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from vector_db_utils import SimpleDocumentIndexer\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08231b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object ChromaDBVectorMemory.clear at 0x7f1d3fd884f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"rag_memory\",\n",
    "        persist_directory=os.getenv(\"CHROMA_PERSIST_DIRECTORY\", \"chroma_db\"),\n",
    "        embedding_model=os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\"),\n",
    "        vector_search_config={\n",
    "            \"k\": 5,\n",
    "            \"distance_metric\": \"cosine\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "rag_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82c96f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: module 'chromadb' has no attribute 'get_settings'\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: module 'chromadb' has no attribute 'get_settings'\n",
      "/home/vscode/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:10<00:00, 7.58MiB/s]\n",
      "Failed to send telemetry event CollectionAddEvent: module 'chromadb' has no attribute 'get_settings'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 47 chunks from documents and sources into the RAG memory.\n"
     ]
    }
   ],
   "source": [
    "async def index_documents() -> None:\n",
    "    indexer = SimpleDocumentIndexer(\n",
    "        memory=rag_memory,\n",
    "    )\n",
    "\n",
    "    sources = [\n",
    "        \"https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md\"\n",
    "        \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html\",\n",
    "        \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html\",\n",
    "        \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html\"\n",
    "    ]\n",
    "\n",
    "    chunks:int = await indexer.index_documents(sources)\n",
    "    \n",
    "    print(f\"Indexed {chunks} chunks from documents and sources into the RAG memory.\")\n",
    "\n",
    "await index_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5453b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c7d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is AgentChat in AutoGen?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: module 'chromadb' has no attribute 'get_settings'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- MemoryQueryEvent (RAGAssistant) ----------\n",
      "[MemoryContent(content='arch Literature Review API Reference PyPi Source AgentChat Agents Agents # AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods: name : The unique name of the agent. description : The description of the agent in text. run : The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult . Agents are expected to be stateful and this method is expected to be called with new messages, not complete history . run_stream : Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item. See autogen_agentchat.messages for more information on AgentChat message types. Assistant Agent # AssistantAgent is a built-in agent that uses a language model and has the ability to use tools. Warning AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent . from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import StructuredMessage from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Define a tool that searches the web for information. # For simplicity,', mime_type='MemoryMimeType.TEXT', metadata={'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'chunk_index': 1, 'mime_type': 'MemoryMimeType.TEXT', 'score': 0.6200529932975769, 'id': '78accb1d-95a5-41d5-bb36-1a85c009ee40'}), MemoryContent(content='multi-agent applications.&#39;, name=&#39;web_search&#39;, call_id=&#39;call_703i17OLXfztkuioUbkESnea&#39;, is_error=False)], type=&#39;ToolCallExecutionEvent&#39;), ToolCallSummaryMessage(source=&#39;assistant&#39;, models_usage=None, metadata={}, content=&#39;AutoGen is a programming framework for building multi-agent applications.&#39;, type=&#39;ToolCallSummaryMessage&#39;)] The call to the run() method returns a TaskResult with the list of messages in the messages attribute, which stores the agent’s “thought process” as well as the final response. Note It is important to note that run() will update the internal state of the agent – it will add the messages to the agent’s message history. You can also call run() without a task to get the agent to generate responses given its current state. Note Unlike in v0.2 AgentChat, the tools are executed by the same agent directly within the same call to run() . By default, the agent will return the result of the tool call as the final response. Multi-Modal Input # The AssistantAgent can handle multi-modal input by providing the input as a MultiModalMessage . from io import BytesIO import PIL import requests from autogen_agentchat.messages import MultiModalMessage from autogen_core import Image # Create a multi-modal message with random image and text. pil_image = PIL . Image . open ( BytesIO ( requests . get ( &quot;https://picsum.photos/300/200&quot; ) . content )) img = Image ( pil_image ) multi_modal_message = MultiModalMessage', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 3, 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.5928937792778015, 'id': '92660591-4a6e-4fe6-839d-6b18767f3b38'}), MemoryContent(content='Creating a Team # RoundRobinGroupChat is a simple yet effective team configuration where all agents share the same context and take turns responding in a round-robin fashion. Each agent, during its turn, broadcasts its response to all other agents, ensuring that the entire team maintains a consistent context. We will begin by creating a team with two AssistantAgent and a TextMentionTermination condition that stops the team when a specific word is detected in the agent’s response. The two-agent team implements the reflection pattern, a multi-agent design pattern where a critic agent evaluates the responses of a primary agent. Learn more about the reflection pattern using the Core API . import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.base import TaskResult from autogen_agentchat.conditions import ExternalTermination , TextMentionTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient # Create an OpenAI model client. model_client = OpenAIChatCompletionClient ( model = &quot;gpt-4o-2024-08-06&quot; , # api_key=&quot;sk-...&quot;, # Optional if you have an OPENAI_API_KEY env variable set. ) # Create the primary agent. primary_agent = AssistantAgent ( &quot;primary&quot; , model_client = model_client , system_message = &quot;You are a helpful AI assistant.&quot; , ) # Create the cri', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html', 'chunk_index': 2, 'score': 0.5115514099597931, 'id': 'a81fb087-4327-4801-969b-9e0a1d8d2e53'})]\n",
      "---------- TextMessage (RAGAssistant) ----------\n",
      "AgentChat in AutoGen is a component designed for creating and managing chat-based agents. It provides a set of preset agents with variations in how they respond to messages. These agents are defined by key attributes and methods:\n",
      "\n",
      "- **Attributes**:\n",
      "  - **name**: A unique identifier for the agent.\n",
      "  - **description**: A textual description of the agent's capabilities and purpose.\n",
      "\n",
      "- **Methods**:\n",
      "  - **run**: This method executes the agent's tasks based on a string input or a list of messages, and it returns a `TaskResult`. The agent maintains state and updates its internal message history each time this method is called.\n",
      "  - **run_stream**: Similar to `run()`, but this method returns an iterator of messages that extend from `BaseAgentEvent` or `BaseChatMessage`, concluding with a `TaskResult`.\n",
      "\n",
      "AgentChat also supports multi-modal input, which allows the agent to process and respond to various types of input, including text and images.\n",
      "\n",
      "A notable built-in agent is the **AssistantAgent**, which leverages a language model and can use various tools to assist in answering queries or completing tasks. It is referred to as a \"kitchen sink\" agent ideal for prototyping and educational purposes.\n",
      "\n",
      "This framework is focused on building and managing stateful, multi-agent applications, allowing complex interactions where agents can cooperate and communicate to fulfill tasks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 6, 18, 21, 55, 163396, tzinfo=datetime.timezone.utc), content='What is AgentChat in AutoGen?', type='TextMessage'), MemoryQueryEvent(source='RAGAssistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 6, 18, 21, 55, 717537, tzinfo=datetime.timezone.utc), content=[MemoryContent(content='arch Literature Review API Reference PyPi Source AgentChat Agents Agents # AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods: name : The unique name of the agent. description : The description of the agent in text. run : The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult . Agents are expected to be stateful and this method is expected to be called with new messages, not complete history . run_stream : Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item. See autogen_agentchat.messages for more information on AgentChat message types. Assistant Agent # AssistantAgent is a built-in agent that uses a language model and has the ability to use tools. Warning AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent . from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import StructuredMessage from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Define a tool that searches the web for information. # For simplicity,', mime_type='MemoryMimeType.TEXT', metadata={'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'chunk_index': 1, 'mime_type': 'MemoryMimeType.TEXT', 'score': 0.6200529932975769, 'id': '78accb1d-95a5-41d5-bb36-1a85c009ee40'}), MemoryContent(content='multi-agent applications.&#39;, name=&#39;web_search&#39;, call_id=&#39;call_703i17OLXfztkuioUbkESnea&#39;, is_error=False)], type=&#39;ToolCallExecutionEvent&#39;), ToolCallSummaryMessage(source=&#39;assistant&#39;, models_usage=None, metadata={}, content=&#39;AutoGen is a programming framework for building multi-agent applications.&#39;, type=&#39;ToolCallSummaryMessage&#39;)] The call to the run() method returns a TaskResult with the list of messages in the messages attribute, which stores the agent’s “thought process” as well as the final response. Note It is important to note that run() will update the internal state of the agent – it will add the messages to the agent’s message history. You can also call run() without a task to get the agent to generate responses given its current state. Note Unlike in v0.2 AgentChat, the tools are executed by the same agent directly within the same call to run() . By default, the agent will return the result of the tool call as the final response. Multi-Modal Input # The AssistantAgent can handle multi-modal input by providing the input as a MultiModalMessage . from io import BytesIO import PIL import requests from autogen_agentchat.messages import MultiModalMessage from autogen_core import Image # Create a multi-modal message with random image and text. pil_image = PIL . Image . open ( BytesIO ( requests . get ( &quot;https://picsum.photos/300/200&quot; ) . content )) img = Image ( pil_image ) multi_modal_message = MultiModalMessage', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 3, 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.5928937792778015, 'id': '92660591-4a6e-4fe6-839d-6b18767f3b38'}), MemoryContent(content='Creating a Team # RoundRobinGroupChat is a simple yet effective team configuration where all agents share the same context and take turns responding in a round-robin fashion. Each agent, during its turn, broadcasts its response to all other agents, ensuring that the entire team maintains a consistent context. We will begin by creating a team with two AssistantAgent and a TextMentionTermination condition that stops the team when a specific word is detected in the agent’s response. The two-agent team implements the reflection pattern, a multi-agent design pattern where a critic agent evaluates the responses of a primary agent. Learn more about the reflection pattern using the Core API . import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.base import TaskResult from autogen_agentchat.conditions import ExternalTermination , TextMentionTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient # Create an OpenAI model client. model_client = OpenAIChatCompletionClient ( model = &quot;gpt-4o-2024-08-06&quot; , # api_key=&quot;sk-...&quot;, # Optional if you have an OPENAI_API_KEY env variable set. ) # Create the primary agent. primary_agent = AssistantAgent ( &quot;primary&quot; , model_client = model_client , system_message = &quot;You are a helpful AI assistant.&quot; , ) # Create the cri', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html', 'chunk_index': 2, 'score': 0.5115514099597931, 'id': 'a81fb087-4327-4801-969b-9e0a1d8d2e53'})], type='MemoryQueryEvent'), TextMessage(source='RAGAssistant', models_usage=RequestUsage(prompt_tokens=1009, completion_tokens=280), metadata={}, created_at=datetime.datetime(2025, 6, 6, 18, 22, 44, 189855, tzinfo=datetime.timezone.utc), content='AgentChat in AutoGen is a component designed for creating and managing chat-based agents. It provides a set of preset agents with variations in how they respond to messages. These agents are defined by key attributes and methods:\\n\\n- **Attributes**:\\n  - **name**: A unique identifier for the agent.\\n  - **description**: A textual description of the agent\\'s capabilities and purpose.\\n\\n- **Methods**:\\n  - **run**: This method executes the agent\\'s tasks based on a string input or a list of messages, and it returns a `TaskResult`. The agent maintains state and updates its internal message history each time this method is called.\\n  - **run_stream**: Similar to `run()`, but this method returns an iterator of messages that extend from `BaseAgentEvent` or `BaseChatMessage`, concluding with a `TaskResult`.\\n\\nAgentChat also supports multi-modal input, which allows the agent to process and respond to various types of input, including text and images.\\n\\nA notable built-in agent is the **AssistantAgent**, which leverages a language model and can use various tools to assist in answering queries or completing tasks. It is referred to as a \"kitchen sink\" agent ideal for prototyping and educational purposes.\\n\\nThis framework is focused on building and managing stateful, multi-agent applications, allowing complex interactions where agents can cooperate and communicate to fulfill tasks.', type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_assistant = AssistantAgent(\n",
    "    name=\"RAGAssistant\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful assistant that can answer questions based on the provided documents.\",\n",
    "    description=\"An assistant that uses RAG to answer questions based on indexed documents.\",\n",
    "    memory=[rag_memory],\n",
    ")\n",
    "\n",
    "stream = rag_assistant.run_stream(task=\"What is AgentChat in AutoGen?\")\n",
    "\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f35cab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "await rag_memory.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
